# Part of the code for this project was generated by ChatGPT.

#pip install seaborn scikit-learn transformers torch nltk

#Importing modules
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.feature_extraction.text import CountVectorizer
import numpy as np
import nltk
from nltk.tokenize import word_tokenize
nltk.download('punkt')
from transformers import GPT2Tokenizer, GPT2LMHeadModel
import torch

#Retrieving the data 
#SST
#train_data = pd.read_csv('Datasæt final/Original_train.csv')
#train_data = pd.read_csv('Datasæt final/sst_chatgpt.csv')
#train_data = pd.read_csv('Datasæt final/sst_paraphrase_05.csv')

#COLA
#train_data = pd.read_csv('Datasæt Cola/cola_train.csv')
#train_data = pd.read_csv('Datasæt Cola/cola_paraphrase05.csv')
#train_data = pd.read_csv('Datasæt Cola/cola_chatgpt.csv')



# EDA
# Distribution of the target variable
count_label_1 = sum(train_data['label'] == 1)
count_label_0 = sum(train_data['label'] == 0)

print(f"Count of labels equal to 1: {count_label_1}")
print(f"Count of labels equal to 0: {count_label_0}")

sns.histplot(train_data['label'])
plt.title('Distribution of Label')
plt.show()

# Distribution of character length in sentences
train_data['text_length'] = train_data['sentence'].apply(len)
statistics = train_data['text_length'].describe()

plt.figure(figsize=(10, 6))
train_data['text_length'].hist(bins=30, edgecolor='k')
plt.title('Distribution of character length in sentences')
plt.xlabel('Characters')
plt.ylabel('Frequency')
plt.show()

print("Descriptive Statistics for Sentence Lengths:")
print(statistics)


# Calculating Average Word Length for each sentence and overall average
train_data['average_word_length'] = train_data['sentence'].apply(lambda x: np.mean([len(word) for word in x.split()]))

# Visualising the distribution of average word lengths
plt.figure(figsize=(10, 6))
train_data['average_word_length'].hist(bins=30, edgecolor='k')
plt.title('Distribution of Average Word Lengths in Sentences')
plt.xlabel('Average Word Length')
plt.ylabel('Frequency')
plt.show()

print("Descriptive Statistics for average word lengths:")
print(train_data['average_word_length'].describe())

# Calculating Word Counts 
train_data['word_count'] = train_data['sentence'].apply(lambda x: len(x.split()))

# Visualising the distribution of word counts
plt.figure(figsize=(10, 6))
plt.hist(train_data['word_count'], bins=30, edgecolor='k')
plt.title('Distribution of Word Counts in Sentences')
plt.xlabel('Word Count')
plt.ylabel('Frequency')
plt.grid(axis='y', alpha=0.75)
plt.show()

print(train_data['word_count'].describe())


#N-GRAM SECTION

# Calulating N-grams
def calculate_ngrams(text_series, n):
    vectorizer = CountVectorizer(ngram_range=(n, n), analyzer='word')
    X = vectorizer.fit_transform(text_series)
    ngrams = vectorizer.get_feature_names_out()
    counts = X.sum(axis=0).A1
    return pd.DataFrame(list(zip(ngrams, counts)), columns=['ngram', 'count']).sort_values(by='count', ascending=False)

# Selecting top 10 n-grams
def create_top_ngrams_dataframe(data, column, n):
    ngrams_df = calculate_ngrams(data[column], n).head(10)
    return ngrams_df

# Plotting the top 10 n-grams for each n_gram
def plot_ngrams(ngrams_df, n):
    plt.figure(figsize=(10, 5))
    plt.bar(ngrams_df['ngram'], ngrams_df['count'], edgecolor='k')
    plt.xlabel('N-grams')
    plt.ylabel('Frequency')
    plt.title(f'Top 10 {n}-grams')
    plt.xticks(rotation=45, ha='right')
    plt.tight_layout()
    plt.show()

# Max 4 n-grams
for n in range(1, 5):
    top_ngrams_df = create_top_ngrams_dataframe(train_data, 'sentence', n)
    print(f"Top 10 {n}-grams:")
    print(top_ngrams_df)
    plot_ngrams(top_ngrams_df, n) 

# N-gram visualization
import pandas as pd
from sklearn.feature_extraction.text import CountVectorizer
import matplotlib.pyplot as plt

# Calculating n_grams
def calculate_ngrams(text_series, n):
    vectorizer = CountVectorizer(ngram_range=(n, n), analyzer='word')
    X = vectorizer.fit_transform(text_series)
    ngrams = vectorizer.get_feature_names_out()
    counts = X.sum(axis=0).A1
    return pd.DataFrame(list(zip(ngrams, counts)), columns=['ngram', 'count']).sort_values(by='count', ascending=False)

colors = ['red', 'green', 'blue', 'yellow']  # Colors for each n-gram size

# Selecting top 4 n-grams
for n in range(1, 5):
    plt.figure(figsize=(10, 5))  # Set up a new figure for each n-gram size
    top_ngrams_df = create_top_ngrams_dataframe(train_data, 'sentence', n)
    plt.scatter([1] * len(top_ngrams_df), top_ngrams_df['count'], color=colors[n-1], label=f'{n}-grams')
    plt.xlabel('N-gram')
    plt.ylabel('Frequency')
    plt.title(f'Frequency of Top 10 {n}-grams')
    plt.xticks([])  # Remove x-axis ticks since they're not meaningful here
    plt.legend()
    plt.tight_layout()
    plt.show()

# TTR Function
def calculate_ttr(sentence):
    tokens = word_tokenize(sentence) 
    types = set(tokens)
    ttr = len(types) / len(tokens)  # Calculate TTR
    return ttr

train_data['TTR'] = train_data['sentence'].apply(calculate_ttr)
mean_ttr = train_data['TTR'].mean()  

print("Mean TTR for the Dataset", mean_ttr)

# Perplexity function using GPT-2
def calculate_perplexity_gpt2(text, tokenizer, model):
    tokens = tokenizer.encode(text, return_tensors="pt")
    device = next(model.parameters()).device
    tokens = tokens.to(device)

    with torch.no_grad():
        outputs = model(tokens, labels=tokens)
        loss = outputs[0] if isinstance(outputs, tuple) else outputs.loss
        perplexity = torch.exp(loss).item()
    return perplexity

# Initializing tokenizer and model
def add_perplexity_column(df):
    tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
    model = GPT2LMHeadModel.from_pretrained("gpt2").eval()
    
    # Calculate perplexity for each sentence
    perplexities = df['sentence'].apply(lambda x: calculate_perplexity_gpt2(x, tokenizer, model))
    
    # Adding perplexity values as a new column 
    df['Perplexity'] = perplexities
    return df

train_data_with_perplexity = add_perplexity_column(train_data)

# Perplexity values
print(train_data['Perplexity'].describe())